# OnlyFAANGs â€” 24 Ã— 7 Job-Scraping & Notification Pipeline

*A cloud-hosted FastAPI service + Chrome extension that tracks tech-company roles in real time.*

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.11-blue?logo=python">
  <img src="https://img.shields.io/badge/FastAPI-%F0%9F%9A%80%20fast-green">
  <img src="https://img.shields.io/badge/Postgres-cloud%20RDS-blue?logo=postgresql">
  <img src="https://img.shields.io/badge/AWS-Elastic%20Beanstalk-orange?logo=amazonaws">
  <img src="https://img.shields.io/badge/Chrome%20Ext-MV3-yellow?logo=googlechrome">
</p>

---

## âœ¨ Features

|                              |                                                           |
| ---------------------------- | --------------------------------------------------------- |
| ğŸ”„ **Autonomous scraping**   | Amazon, Google, and any Greenhouse board list you provide |
| âš¡ **Realtime API**           | Single endpoint â†’ `/jobs/latest?limit=50`                 |
| ğŸ”” **Desktop notifications** | Chrome extension polls every 15 min & fires native toasts |
| ğŸŸ¥ **â€œNewâ€ badge**           | Jobs younger than 60 min glow red in the popup            |
| âœ… **â€œAppliedâ€ checkbox**     | Track applications; state lives in `chrome.storage`       |
| ğŸ“¦ **Dockerised**            | Identical dev/prod images                                 |
| â˜ï¸ **Always-on**             | Runs 24 Ã— 7 on AWS Elastic Beanstalk + PostgreSQL RDS     |
| ğŸ”’ **Secrets-safe**          | `.env` + EB env-vars; nothing private in Git              |

---


## ğŸ›  Tech Stack

| Layer       | Technology                           | Rationale                                   |
| ----------- | ------------------------------------ | ------------------------------------------- |
| Browser     | Manifest V3 Chrome extension         | Native notifications; zero install friction |
| API         | **FastAPI + Uvicorn**                | Async, type-safe, autogenerated docs        |
| Scraping    | Python `requests` modules            | Simple, fast, easy to extend                |
| Scheduling  | **APScheduler**                      | Lightweight cron inside the container       |
| Persistence | **PostgreSQL (AWS RDS)**             | Managed, durable, scalable                  |
| Container   | **Docker** (`python:3.11-slim`)      | Works the same everywhere                   |
| Cloud       | **AWS Elastic Beanstalk (t3.micro)** | One-command deploy, health checks           |

---

## ğŸš€ Quick Start (local dev)

```bash
git clone https://github.com/YOUR_USER/OnlyFAANGs.git
cd OnlyFAANGs

python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

cp .env.example .env     # fill DATABASE_URL & API templates
uvicorn app.main:app --reload
# load extension: chrome://extensions â†’ â€œLoad unpackedâ€ â†’ ./extension
# browse: http://127.0.0.1:8000/jobs/latest?limit=10
```

**Docker dev**

```bash
docker build -t onlyfaangs .
docker run -p 8080:8080 --env-file .env onlyfaangs
```

---

## â˜ï¸ Cloud Deployment (AWS EB)

```bash
eb init -p docker onlyfaangs --region us-east-1
eb setenv DATABASE_URL='postgresql+psycopg2://user:pass@host:5432/db'
eb create onlyfaangs-env --single --instance_type t3.micro
eb open   # view live API
```

---

## ğŸ” Config & Secrets

| Where                               | Purpose                                                         |
| ----------------------------------- | --------------------------------------------------------------- |
| `.env` (git-ignored)                | `DATABASE_URL`, scraping API templates, Greenhouse company list |
| `app/config.py`                     | Loads env-vars â†’ feeds scrapers                                 |
| `extension/config.js` (git-ignored) | Holds production API URL for the extension                      |

Nothing sensitive is committed to GitHub.

---

## â• Add a New Scraper in 3 Steps

1. Create `app/scrapers/leverd.py` with a `fetch()` that returns dicts like
   `{id, company, title, location, url, posted}`.
2. Export it in `scrapers/__init__.py`.
3. Append it to `SCRAPER_FUNCS` in `scheduler.py` â€” redeploy.
   No other code changes required.

---

## ğŸ›¤ Roadmap

* Greenhouse modules
* React dashboard with filters & stats
* CI/CD via GitHub Actions â†’ automatic EB deploy
* HTTPS + custom domain (`jobs.<yourdomain>.com`)

---

## License

MIT â€” free to fork, build, and improve âœ¨

> **OnlyFAANGs** Â© 2025 Muhammad Hunain Khurram â€” PRs & issues welcome!
